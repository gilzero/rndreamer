

**Frontend API Response Specification for Streaming Chat**

**1. Introduction**

This document outlines the expected response format from the backend API for the React Native chat application's streaming chat feature. This specification allows backend developers to create compatible backends in any programming language, ensuring seamless integration with the frontend.

**2. Overview**

The frontend uses the `EventSource` API (provided by the `react-native-sse` library) to establish a persistent connection with the backend for real-time, streaming chat responses. The backend must respond with data formatted according to the Server-Sent Events (SSE) protocol.

**3. Endpoint**

The frontend makes `POST` requests to the following endpoint structure:

`/chat/{provider}`

Where `{provider}` is one of the following:

*   `gpt`: For OpenAI's GPT models.
*   `claude`: For Anthropic's Claude models.
*   `gemini`: For Google's Gemini models.

The base URL is determined dynamically within the frontend based on environment variables (either a development URL like `http://localhost:3050` or a production URL).

**4. Request Body**

The frontend sends a JSON payload in the request body with the following structure:

```json
{
  "messages": [
    {
      "role": "user" | "assistant" | "system",
      "content": "string",
      "timestamp": number,
      "model": "string"
    },
    ... more messages
  ],
  "model": "string"
}
```

*   **`messages`**: An array of chat message objects representing the conversation history.
    *   **`role`**: The role of the message sender ("user", "assistant", or "system").
    *   **`content`**: The text content of the message.
    *   **`timestamp`**: *Optional*. Unix timestamp (milliseconds) of when the message was sent.
    *   **`model`**: *Optional*. The model that generated this message.
*   **`model`**: *(Optional)* The specific model to use within the chosen provider (e.g., "gpt-4o", "claude-3-5-sonnet-latest"). If omitted, the backend should use a default model for the provider.

**5. Response Headers**

The backend *must* include the following HTTP headers in its response to enable streaming:

*   **`Content-Type: text/event-stream`**: This header indicates that the response is an SSE stream.
*   **`Connection: keep-alive`**: This header keeps the connection open for the duration of the stream.
*   **`Cache-Control: no-cache`**: This header prevents caching of the stream.

**6. Response Body (SSE Format)**

The response body must follow the Server-Sent Events (SSE) format. Each "event" in the stream is a block of text lines, separated by double newlines (`\n\n`). Each line within an event is of the form `field: value`.

The frontend expects two types of events:

**6.1 Data Events (Tokens)**

These events contain the individual tokens (pieces of text) generated by the AI model. Each data event *must* have a `data` field containing a JSON string. The JSON string, when parsed, should have the following structure:

```json
{
  "id": "string",
  "delta": {
    "content": "string"
  }
}
```

*   **`id`**: A unique identifier for the message being streamed.  This allows the frontend to correctly assemble the complete message from multiple tokens.
*   **`delta`**: An object containing the change in the message.
    *   **`content`**: The text content of the current token.

**Example:**

```
data: {"id": "message-123", "delta": {"content": "Hello"}}

data: {"id": "message-123", "delta": {"content": " world"}}

data: {"id": "message-123", "delta": {"content": "!"}}

```

**6.2 Done Event**

This event signals the end of the stream. It *must* have a `data` field with the value `[DONE]`.

**Example:**

```
data: [DONE]

```

**7. Complete Example Stream**

```
data: {"id": "message-456", "delta": {"content": "This"}}

data: {"id": "message-456", "delta": {"content": " is"}}

data: {"id": "message-456", "delta": {"content": " a"}}

data: {"id": "message-456", "delta": {"content": " streaming"}}

data: {"id": "message-456", "delta": {"content": " response."}}

data: [DONE]

```

**8. Error Handling**

If an error occurs on the backend, the backend should:

1.  **Send an error event (optional but recommended):** Before closing the connection, send a final data event with an `error` field in the JSON payload. This allows the frontend to display a user-friendly error message.

    ```
    data: {"error": "An error occurred while processing your request."}

    ```

2.  **Close the connection:** The backend should then close the connection. The `EventSource` object on the frontend will detect this and trigger an `error` event.

3.  **Return an appropriate HTTP status code:** While the streaming response itself starts with a 200 OK status, if an error occurs *before* the stream starts (e.g., invalid input), the backend should return an appropriate error status code (e.g., 400 Bad Request, 500 Internal Server Error) *without* setting the `text/event-stream` content type.

**9. Example (Python with Flask)**

```python
from flask import Flask, Response, stream_with_context, request, jsonify
import json
import time

app = Flask(__name__)

@app.route('/chat/<provider>', methods=['POST'])
def chat(provider):
    data = request.get_json()
    messages = data.get('messages', [])
    model = data.get('model', 'default-model')  # Replace with your model logic

    def generate():
        message_id = str(time.time())  # Simple ID for demonstration
        try:
            # Simulate a streaming response from an AI model
            response_chunks = ["This", " is", " a", " simulated", " streaming", " response."]
            for chunk in response_chunks:
                time.sleep(0.5)  # Simulate delay
                yield f"data: {json.dumps({'id': message_id, 'delta': {'content': chunk}})}\n\n"

            yield "data: [DONE]\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"

    return Response(stream_with_context(generate()), mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(debug=True, port=5000)

```